Running out of disk space or encountering Java heap space errors while processing large datasets like 4 million rows in Spark with limited resources (24 GB RAM and 20 GB disk space) can be challenging. Here are some strategies to manage these issues:
1. Optimizing Memory Usage
    a. Increase Executor Memory:

        Adjust the memory allocated to each Spark executor to make better use of the available 24 GB of RAM.
        Set spark.executor.memory to an appropriate value, leaving some overhead for the system.


    val spark = SparkSession.builder()
      .appName("Batch Processing with Optimized Memory")
      .config("spark.executor.memory", "20g")  // Allocate 20 GB to executors
      .config("spark.driver.memory", "4g")     // Allocate 4 GB to the driver
      .config("spark.memory.fraction", "0.6")  // Tune memory fraction
      .getOrCreate()


    b. Garbage Collection Tuning:

        Tuning garbage collection can reduce the frequency of heap space errors.
        Use the G1GC garbage collector, which is designed to handle large heap sizes more efficiently.

    scala

    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35")

    c. Data Persistence:

        If you can afford a bit more time for the processing, consider using lower-memory storage levels like DISK_ONLY for persisting data between operations.

    scala

    rdd.persist(StorageLevel.DISK_ONLY)

2. Optimizing Disk Usage
    a. Use checkpoint with a Custom Directory:

        Use Spark's checkpoint feature to write intermediate results to disk, freeing up memory.
        Set a custom directory for checkpointing with sufficient space.

    scala

    spark.sparkContext.setCheckpointDir("/path/to/larger/disk")

    scala

    rdd.checkpoint()

    b. Reduce spark.local.dir Space Usage:

        Spark stores temporary data in spark.local.dir. You can set this to a directory on a disk with more space or clean it up periodically.

    scala

    .config("spark.local.dir", "/path/to/larger/disk")

    Also, reduce the disk usage by frequently clearing temporary data:

    scala

    spark.conf.set("spark.cleaner.ttl", "3600")  // Set to 1 hour

3. Batch Processing Using RDDs

If you're still encountering issues, consider adjusting your batch processing approach with the following RDD strategy:
Example Code: Batch Processing with RDD

import org.apache.spark.sql.SparkSession
import java.io.{BufferedWriter, FileWriter}

object BatchProcessWithRDD extends App {

  // Create SparkSession
  val spark = SparkSession.builder()
    .appName("Batch Process RDD")
    .config("spark.executor.memory", "20g")  // Adjust as per available memory
    .config("spark.driver.memory", "4g")
    .config("spark.memory.fraction", "0.6")
    .getOrCreate()

  // Create a large RDD
  val rdd = spark.sparkContext.parallelize(1 to 4000000).map(i => s"String $i")

  // Define batch size
  val batchSize = 100000

  // File path to write data
  val filePath = "output_file.txt"

  // Open BufferedWriter
  val writer = new BufferedWriter(new FileWriter(filePath, true))

  try {
    var remainingRDD = rdd

    while (!remainingRDD.isEmpty()) {
      // Take a batch from RDD
      val batchRDD = remainingRDD.zipWithIndex().filter { case (_, idx) => idx < batchSize }.map(_._1)

      // Remove batch from remaining RDD
      remainingRDD = remainingRDD.subtract(batchRDD)

      // Write batch to file
      val stringData = batchRDD.collect().mkString("\n")
      writer.write(stringData + "\n")
      writer.flush()

      // Force GC after each batch to free memory
      System.gc()
    }
  } finally {
    writer.close()
  }

  // Stop Spark session
  spark.stop()
}


Key Points in the RDD Approach

    zipWithIndex(): Allows indexing the RDD elements, making it possible to simulate batch processing.
    subtract(): Efficiently removes the processed batch from the main RDD.
    System.gc(): Explicitly invokes garbage collection after each batch to clear memory.
    Disk Management: Use a path for spark.local.dir or checkpoint that has sufficient space to store intermediate results.

4. Consider Writing Directly to Disk

For extremely large datasets, you might need to write intermediate results directly to disk in smaller files and merge them later to reduce memory pressure.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Persist RDDs Efficiently

    Storage Levels: Use appropriate storage levels when persisting RDDs. For example, use MEMORY_AND_DISK_SER to serialize the RDD and store it in memory or disk, reducing memory usage.

    rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

Unpersist When Done: Ensure you unpersist RDDs after they are no longer needed to free up memory.

scala

rdd.unpersist()


3. Process in Smaller Batches

Instead of processing the entire dataset at once, split the data into smaller, more manageable batches. This approach helps avoid running out of memory and disk space.
a. Use zipWithIndex to Batch RDD Processing

You can use zipWithIndex to add an index to each element and then filter by index to create smaller batches:

val batchSize = 100000
val indexedRDD = rdd.zipWithIndex()

for (i <- 0L until (rdd.count() / batchSize)) {
  val batchRDD = indexedRDD.filter {
    case (_, index) => index >= i * batchSize && index < (i + 1) * batchSize
  }.map(_._1) // Remove index
  // Process batchRDD
}


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import org.apache.spark.sql.SparkSession
import java.io.{BufferedWriter, FileWriter}

object BatchProcessRDD extends App {

  // 1. Create a SparkSession
  val spark = SparkSession.builder()
    .appName("Batch Process RDD with Resource Management")
    .master("local[*]") // Adjust based on your environment
    .config("spark.executor.memory", "8g")
    .config("spark.driver.memory", "4g")
    .config("spark.memory.fraction", "0.6")
    .config("spark.memory.storageFraction", "0.3")
    .config("spark.local.dir", "/path/to/tmp") // Ensure this path has sufficient space
    .getOrCreate()

  import spark.implicits._

  // 2. Create an RDD with 4 million records
  var rdd = spark.sparkContext.parallelize(1 to 4000000).map(i => s"String $i")

  // 3. Define batch size (adjusted for memory constraints)
  val batchSize = 10000

  // 4. Define the file path where the data will be written
  val filePath = "output_file.txt"

  // Open the file in append mode
  val writer = new BufferedWriter(new FileWriter(filePath, true))

  try {
    while (!rdd.isEmpty()) {
      // 5. Process a batch
      val batch = rdd.take(batchSize)

      // 6. Remove the processed batch from the RDD
      val batchRDD = spark.sparkContext.parallelize(batch)
      rdd = rdd.subtract(batchRDD)

      // 7. Write the batch to a file
      val stringData = batch.mkString("\n")
      writer.write(stringData + "\n")
      writer.flush() // Ensure data is written to the file immediately

      // Clean up to avoid filling up disk space
      spark.catalog.clearCache()
      System.gc()
    }
  } finally {
    // Close the writer after processing
    writer.close()
  }

  // 8. Stop the Spark session
  spark.stop()
}

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import java.io.{BufferedWriter, FileWriter}

val spark = SparkSession.builder()
  .appName("Batch Processing Example")
  .config("spark.executor.memory", "20g")
  .config("spark.driver.memory", "20g")
  .config("spark.memory.fraction", "0.6")
  .config("spark.memory.storageFraction", "0.3")
  .getOrCreate()

spark.conf.set("spark.sql.shuffle.partitions", "400")

// Example DataFrame creation (replace with actual data loading code)
import spark.implicits._
val df = spark.range(0, 4000000).map(_.toString).toDF("column_name")


val batchSize = 100000
val outputFilePath = "/path/to/output/file.txt"

df.foreachPartition { partition =>
  // Use a BufferedWriter to write data to file
  val writer = new BufferedWriter(new FileWriter(outputFilePath, true))

  try {
    // Process each batch
    partition.grouped(batchSize).foreach { batch =>
      val stringData = batch.map(_.getString(0)).mkString("\n")
      writer.write(stringData + "\n")
    }
  } finally {
    writer.close()
  }
}


spark.conf.set("spark.memory.fraction", "0.6")
spark.conf.set("spark.memory.storageFraction", "0.3")


--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication"
--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication"


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To write a single-column `DataFrame` with 4 million records to a flat file in a batchwise manner while avoiding heap space errors, we can use the following approach:

### Key Considerations

1. **Batch Processing**: Process and write the data in manageable batches to avoid excessive memory usage.
2. **Efficient Memory Management**: Use Spark's built-in functionality to handle large datasets without overwhelming the available memory.
3. **Resource Configuration**: Ensure that Spark's memory settings are configured appropriately to avoid `OutOfMemoryError`.

### Steps to Implement the Solution

#### 1. **Spark Configuration**
   - **Executor Memory**: Set a reasonable amount of memory for the executor, considering that you have 24 GB of RAM.
   - **Disk Spill**: Configure Spark to spill to disk when memory is insufficient.

Here's how you might configure these settings:

```scala
import org.apache.spark.sql.{SparkSession, DataFrame}
import java.io.{BufferedWriter, FileWriter}

val spark = SparkSession.builder()
  .appName("Batch Write Single Column DataFrame")
  .config("spark.executor.memory", "8g")
  .config("spark.driver.memory", "4g")
  .config("spark.memory.fraction", "0.6")
  .config("spark.memory.storageFraction", "0.3")
  .config("spark.sql.files.maxPartitionBytes", "16MB")
  .config("spark.sql.autoBroadcastJoinThreshold", "-1")
  .getOrCreate()
```

#### 2. **Create or Load Your DataFrame**
   - Assume you have a `DataFrame` with a single column of type `String`.

```scala
import org.apache.spark.sql.functions._
import org.apache.spark.sql.DataFrame

// Example DataFrame with 4 million rows
val df: DataFrame = spark.range(1, 4000001)
  .select(col("id").cast("string").alias("yourColumnName"))
```

#### 3. **Write DataFrame in Batches to a Flat File**
   - Use the `foreachPartition` method to write the data in smaller batches.

```scala
val outputFilePath = "path/to/output/file.txt"
val batchSize = 100000

df.foreachPartition { partition =>
  val fileWriter = new BufferedWriter(new FileWriter(outputFilePath, true))
  try {
    partition.grouped(batchSize).foreach { batch =>
      val stringData = batch.map(_.getString(0)).mkString("\n")
      fileWriter.write(stringData + "\n")
      fileWriter.flush()
    }
  } finally {
    fileWriter.close()
  }
}
```

#### 4. **Adjust Spark Memory and Disk Configuration**
   - Ensure that you have set up Spark to spill intermediate data to disk if necessary, to avoid heap space issues.

```scala
spark.conf.set("spark.sql.shuffle.partitions", "400")
spark.conf.set("spark.storage.memoryFraction", "0.3")
spark.conf.set("spark.memory.offHeap.enabled", "true")
spark.conf.set("spark.memory.offHeap.size", "4g")
```

### Summary of the Approach

1. **Batch Size**: The `batchSize` is set to 100,000 records to keep memory usage within limits. Adjust this size according to your environment's performance.
2. **BufferedWriter**: The `BufferedWriter` is used to write data incrementally to the file, which is efficient for large datasets.
3. **foreachPartition**: This Spark method allows for processing data in partitions, which can be more memory-efficient than processing the entire DataFrame at once.

By implementing the above approach, you should be able to write the large `DataFrame` to a flat file without running into heap space issues, even with the constraints of your environment.


==============================================================================================================

git init
git add index.html
git commit -m "Initial commit"
git branch feature/agile-v1
git checkout feature/agile-v1
vi index.html
git add index.html
git commit -m "Second commit"
git commit --allow-empty-message -m ""
git tag -a v1 -m "Version 1"
git rebase master
git checkout master
git branch feature/agile-v2
git checkout feature/agile-v2
vi index.html
git add index.html
git commit -m "Third commit"
git commit --allow-empty-message -m ""
git tag -a v2 -m "Version 2"
git branch
git checkout feature/agile-v2
git branch feature/agile-v3
git checkout feature/agile-v3
vi index.html
git add index.html
git commit -m "Working software - list"
vi index.html
git add index.html
git commit -m "Continuous improvement - list"
git log
git checkout feature/agile-v2
git cherry-pick <<commit hash for Working software - list>>
git checkout master
git merge feature/agile-v2


=============================================================================================

def process_list(arr):
    # Keep running until the list is empty
    while len(arr) > 0:
        # Find the index of the smallest element
        smallest_idx = 0
        for i in range(1, len(arr)):
            if arr[i] < arr[smallest_idx]:
                smallest_idx = i

        # Print current status
        print(f"Smallest element: {arr[smallest_idx]} at index {smallest_idx}")
        print(f"List before removal: {arr}")

        # Create a new list excluding the smallest element and its neighbors (-1 and +1)
        new_list = []
        for i in range(len(arr)):
            # Keep elements that are not the smallest and not its neighbors
            if i < smallest_idx - 1 or i > smallest_idx + 1:
                new_list.append(arr[i])

        # Update the list
        arr = new_list
        print(f"List after removal: {arr}")
        print('-' * 30)

# Example usage:
arr = [7, 3, 1, 4, 9, 2, 6, 5]
process_list(arr)

=============================================================================================

def find_smallest_index(arr):
    # Find the index of the smallest element in the list
    smallest_idx = 0
    for i in range(1, len(arr)):
        if arr[i] < arr[smallest_idx]:
            smallest_idx = i
    return smallest_idx

def process_list(arr):
    # Keep running until the list is empty
    while len(arr) > 0:
        # Find the index of the smallest element
        smallest_idx = find_smallest_index(arr)

        # Print current status
        print(f"Smallest element: {arr[smallest_idx]} at index {smallest_idx}")
        print(f"List before removal: {arr}")

        # Create a new list excluding the smallest element and its neighbors (-1 and +1)
        new_list = []
        for i in range(len(arr)):
            # Keep elements that are not the smallest and not its neighbors
            if i < smallest_idx - 1 or i > smallest_idx + 1:
                new_list.append(arr[i])

        # Update the list
        arr = new_list
        print(f"List after removal: {arr}")
        print('-' * 30)

# Example usage:
arr = [7, 3, 1, 4, 9, 2, 6, 5]
process_list(arr)
