Running out of disk space or encountering Java heap space errors while processing large datasets like 4 million rows in Spark with limited resources (24 GB RAM and 20 GB disk space) can be challenging. Here are some strategies to manage these issues:
1. Optimizing Memory Usage
    a. Increase Executor Memory:

        Adjust the memory allocated to each Spark executor to make better use of the available 24 GB of RAM.
        Set spark.executor.memory to an appropriate value, leaving some overhead for the system.


    val spark = SparkSession.builder()
      .appName("Batch Processing with Optimized Memory")
      .config("spark.executor.memory", "20g")  // Allocate 20 GB to executors
      .config("spark.driver.memory", "4g")     // Allocate 4 GB to the driver
      .config("spark.memory.fraction", "0.6")  // Tune memory fraction
      .getOrCreate()


    b. Garbage Collection Tuning:

        Tuning garbage collection can reduce the frequency of heap space errors.
        Use the G1GC garbage collector, which is designed to handle large heap sizes more efficiently.

    scala

    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35")

    c. Data Persistence:

        If you can afford a bit more time for the processing, consider using lower-memory storage levels like DISK_ONLY for persisting data between operations.

    scala

    rdd.persist(StorageLevel.DISK_ONLY)

2. Optimizing Disk Usage
    a. Use checkpoint with a Custom Directory:

        Use Spark's checkpoint feature to write intermediate results to disk, freeing up memory.
        Set a custom directory for checkpointing with sufficient space.

    scala

    spark.sparkContext.setCheckpointDir("/path/to/larger/disk")

    scala

    rdd.checkpoint()

    b. Reduce spark.local.dir Space Usage:

        Spark stores temporary data in spark.local.dir. You can set this to a directory on a disk with more space or clean it up periodically.

    scala

    .config("spark.local.dir", "/path/to/larger/disk")

    Also, reduce the disk usage by frequently clearing temporary data:

    scala

    spark.conf.set("spark.cleaner.ttl", "3600")  // Set to 1 hour

3. Batch Processing Using RDDs

If you're still encountering issues, consider adjusting your batch processing approach with the following RDD strategy:
Example Code: Batch Processing with RDD

import org.apache.spark.sql.SparkSession
import java.io.{BufferedWriter, FileWriter}

object BatchProcessWithRDD extends App {

  // Create SparkSession
  val spark = SparkSession.builder()
    .appName("Batch Process RDD")
    .config("spark.executor.memory", "20g")  // Adjust as per available memory
    .config("spark.driver.memory", "4g")
    .config("spark.memory.fraction", "0.6")
    .getOrCreate()

  // Create a large RDD
  val rdd = spark.sparkContext.parallelize(1 to 4000000).map(i => s"String $i")

  // Define batch size
  val batchSize = 100000

  // File path to write data
  val filePath = "output_file.txt"

  // Open BufferedWriter
  val writer = new BufferedWriter(new FileWriter(filePath, true))

  try {
    var remainingRDD = rdd

    while (!remainingRDD.isEmpty()) {
      // Take a batch from RDD
      val batchRDD = remainingRDD.zipWithIndex().filter { case (_, idx) => idx < batchSize }.map(_._1)

      // Remove batch from remaining RDD
      remainingRDD = remainingRDD.subtract(batchRDD)

      // Write batch to file
      val stringData = batchRDD.collect().mkString("\n")
      writer.write(stringData + "\n")
      writer.flush()

      // Force GC after each batch to free memory
      System.gc()
    }
  } finally {
    writer.close()
  }

  // Stop Spark session
  spark.stop()
}


Key Points in the RDD Approach

    zipWithIndex(): Allows indexing the RDD elements, making it possible to simulate batch processing.
    subtract(): Efficiently removes the processed batch from the main RDD.
    System.gc(): Explicitly invokes garbage collection after each batch to clear memory.
    Disk Management: Use a path for spark.local.dir or checkpoint that has sufficient space to store intermediate results.

4. Consider Writing Directly to Disk

For extremely large datasets, you might need to write intermediate results directly to disk in smaller files and merge them later to reduce memory pressure.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Persist RDDs Efficiently

    Storage Levels: Use appropriate storage levels when persisting RDDs. For example, use MEMORY_AND_DISK_SER to serialize the RDD and store it in memory or disk, reducing memory usage.

    rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)

Unpersist When Done: Ensure you unpersist RDDs after they are no longer needed to free up memory.

scala

rdd.unpersist()


3. Process in Smaller Batches

Instead of processing the entire dataset at once, split the data into smaller, more manageable batches. This approach helps avoid running out of memory and disk space.
a. Use zipWithIndex to Batch RDD Processing

You can use zipWithIndex to add an index to each element and then filter by index to create smaller batches:

val batchSize = 100000
val indexedRDD = rdd.zipWithIndex()

for (i <- 0L until (rdd.count() / batchSize)) {
  val batchRDD = indexedRDD.filter {
    case (_, index) => index >= i * batchSize && index < (i + 1) * batchSize
  }.map(_._1) // Remove index
  // Process batchRDD
}


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import org.apache.spark.sql.SparkSession
import java.io.{BufferedWriter, FileWriter}

object BatchProcessRDD extends App {

  // 1. Create a SparkSession
  val spark = SparkSession.builder()
    .appName("Batch Process RDD with Resource Management")
    .master("local[*]") // Adjust based on your environment
    .config("spark.executor.memory", "8g")
    .config("spark.driver.memory", "4g")
    .config("spark.memory.fraction", "0.6")
    .config("spark.memory.storageFraction", "0.3")
    .config("spark.local.dir", "/path/to/tmp") // Ensure this path has sufficient space
    .getOrCreate()

  import spark.implicits._

  // 2. Create an RDD with 4 million records
  var rdd = spark.sparkContext.parallelize(1 to 4000000).map(i => s"String $i")

  // 3. Define batch size (adjusted for memory constraints)
  val batchSize = 10000

  // 4. Define the file path where the data will be written
  val filePath = "output_file.txt"

  // Open the file in append mode
  val writer = new BufferedWriter(new FileWriter(filePath, true))

  try {
    while (!rdd.isEmpty()) {
      // 5. Process a batch
      val batch = rdd.take(batchSize)

      // 6. Remove the processed batch from the RDD
      val batchRDD = spark.sparkContext.parallelize(batch)
      rdd = rdd.subtract(batchRDD)

      // 7. Write the batch to a file
      val stringData = batch.mkString("\n")
      writer.write(stringData + "\n")
      writer.flush() // Ensure data is written to the file immediately

      // Clean up to avoid filling up disk space
      spark.catalog.clearCache()
      System.gc()
    }
  } finally {
    // Close the writer after processing
    writer.close()
  }

  // 8. Stop the Spark session
  spark.stop()
}
