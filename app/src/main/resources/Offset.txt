Writing a large DataFrame with 15 million rows to disk on a single server with 24 GB of RAM and 30 GB of hard drive space requires a memory-efficient approach to avoid exhausting resources. Spark can manage this efficiently, but the key is to minimize memory usage and disk I/O.

Here’s an optimal approach to write the data in a memory-friendly way while ensuring that you don’t overload your server.

### Challenges:
- **Memory Constraints**: You have 24 GB of RAM, but Spark's overhead and other processes will consume part of it.
- **Disk Space**: You need to handle 15 million rows efficiently without filling the 30 GB hard drive.

### Approach:
1. **Partitioning the DataFrame**: Split the DataFrame into smaller partitions to allow for more efficient processing.
2. **Batch Writing**: Write the data in smaller chunks to avoid memory overload.
3. **Optimized Storage Format**: Use an efficient file format like Parquet or ORC to save space on disk.
4. **Avoid Large Shuffles**: Ensure minimal shuffling to prevent memory overload during writes.

### Step-by-Step Solution

#### 1. Configure Spark for Resource Management

Start by configuring your Spark session to limit memory usage and manage resources efficiently.

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Efficient Large DataFrame Writing")
  .config("spark.executor.memory", "20g")  // Allocate a safe amount of RAM
  .config("spark.memory.fraction", "0.6")  // Allocate 60% of memory for execution (leaving some for shuffle space)
  .config("spark.sql.shuffle.partitions", "150")  // Set shuffle partitions for large dataframes
  .getOrCreate()
```

Here, `spark.memory.fraction` controls how much of the executor memory is used for computing versus storage tasks like shuffling data. You can adjust these parameters based on your use case.

#### 2. Repartition the DataFrame

Repartitioning the DataFrame into smaller chunks will allow Spark to process data in a distributed way, minimizing memory pressure. Each partition will be processed independently and written to disk.

```scala
// Assuming df is the DataFrame with 15 million rows
val partitionedDF = df.repartition(150)  // Choose 150 or adjust based on cluster resources
```

By repartitioning, you distribute the data evenly across partitions for more efficient processing.

#### 3. Write Data in Batches

Writing the entire DataFrame at once could overwhelm memory and disk space, so you can write it incrementally in smaller batches. You can process a limited number of rows at a time by using an offset-based approach (or filtering by partition).

Here's an example using a combination of `limit` and `offset` to simulate batch processing:

```scala
val totalRows = df.count()
val batchSize = 1000000  // 1 million rows at a time

for (offset <- 0 until totalRows.toInt by batchSize) {
  val batchDF = df.limit(batchSize).offset(offset)  // Fetching a batch
  batchDF.write
    .mode("append")  // Append data to the file
    .parquet("output/path")  // Use an efficient format like Parquet or ORC
}
```

If `offset` functionality is unavailable directly, you can use `limit` and `filter` based on an indexed column.

#### 4. Use Parquet Format for Efficient Disk Usage

Writing data in a format like Parquet or ORC is much more efficient than using CSV or plain text, especially with large datasets. Parquet compresses data and uses columnar storage, which significantly reduces disk space usage and I/O.

```scala
df.write
  .option("compression", "snappy")  // Use efficient compression
  .parquet("/path/to/output/")  // Write to disk in Parquet format
```

Parquet with Snappy compression will help ensure that the data is written efficiently while minimizing disk space usage.

#### 5. Manage Disk Space During Write

Because you have 30 GB of disk space, it's crucial to keep an eye on how much space is being used during the write process. The choice of file format (Parquet) and compression helps, but you should also monitor disk space usage carefully.

#### 6. Alternative: Write Data in CSV Format (if necessary)

If you must write in a plain format (like CSV), you can still optimize by splitting the DataFrame into smaller chunks to prevent memory and disk overload:

```scala
val partitionedDF = df.repartition(150)  // Repartition into smaller chunks

partitionedDF.write
  .option("header", "true")
  .csv("/path/to/output/")  // Write each partition as a separate file
```

However, CSV will take more disk space and may be slower to write compared to Parquet.

### 7. Monitoring and Tuning

- **Monitor Memory Usage**: Keep an eye on Spark UI to monitor memory usage during writing. Ensure that the batch size is manageable, so you don't run out of memory.
- **Disk Usage**: Monitor the available disk space as data is being written to avoid running out of disk.

### Example Code to Write in Batches

```scala
import java.io.File

// Example of processing a DataFrame in batches to avoid memory overload
val totalRecords = df.count()
val batchSize = 1000000  // 1 million records at a time
var offset = 0L

while (offset < totalRecords) {
  val batchDF = df.limit(batchSize).offset(offset.toInt)

  batchDF.write
    .mode("append")  // Append data to avoid overwriting
    .parquet("/path/to/output/")

  offset += batchSize  // Move to the next batch
}
```

This code snippet processes 1 million rows at a time, appending each batch to the output Parquet file. You can adjust the batch size as needed based on performance and resource availability.

### Summary

- **Use efficient formats**: Parquet with compression (e.g., Snappy) is the most disk-efficient format for writing large datasets.
- **Write in batches**: Process and write data in smaller chunks (e.g., 1 million rows) to prevent memory overload.
- **Repartition**: Split the DataFrame into smaller partitions (e.g., 150) to distribute the processing load.
- **Monitor resources**: Ensure memory and disk space usage is within limits by checking Spark UI and disk usage.

By following this approach, you can write 15 million rows efficiently to disk on a server with 24 GB of RAM and 30 GB of hard drive space without running into memory or disk overload issues.